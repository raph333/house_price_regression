---
title: "Heteroskedastic Ridge Regression"
author: "Philipp Grafendorfer"
date: "16 Januar 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(42)
library(glmnet)
library(psych)
library(dplyr)
library(tibble)
library(caret)
```

Lade Daten
```{r}
X_train <- readRDS("../data/train_X.RDS")
y_train <- readRDS("../data/train_y.RDS")
X_val <- readRDS("../data/val_X.RDS")
y_val <- readRDS("../data/val_y.RDS")
```

```{r}
# check if there is a column with var=0; eliminate that variable from train and val set
nzv <- caret::nearZeroVar(X_train, saveMetrics= TRUE)
zv_names <- nzv %>% 
  rownames_to_column('var') %>% 
  filter(zeroVar==T) %>% 
  .$var

dim_names <- X_train@Dimnames[[2]]
non_zv_names <- base::setdiff(dim_names, zv_names)
X_train <- X_train[,non_zv_names]
X_val <- X_val[,non_zv_names]
```

```{r}
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X_train)), function(predictor) {
  uni_model <- lm(y_train ~ X_train[, predictor])
  coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})
```

```{r}
# provide hridge_loss as function because it has to be optimized later on
# this may look weird but its the most convenient way
hridge_loss <- function(betas) {
  sum((y_train - X_train %*% betas)^2) + lambda * sum(weights * betas^2)
}
```

```{r}
# Heteroskedastic Ridge Regression function
hridge <- function(X, y, lambda, weights) {
  # Use regular ridge regression coefficient as initial values for optimization
  model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
  betas_init <- as.vector(model_init$beta)
  # Solve optimization problem to get coefficients
  coef <- optim(betas_init, hridge_loss)$par
  # Compute fitted values and multiple R-squared
  fitted <- X %*% coef
  rsq <- cor(y, as.vector(fitted))^2
  names(coef) <- dimnames(X)[[2]]
  output <- list("coef" = coef,
                 "fitted" = fitted,
                 "rsq" = rsq)
  return(output)
}
```

```{r}
# Fit model to the data for lambda = 0.001
lambda <- 40000
hridge_model <- hridge(X_train, y_train, lambda = lambda, weights = weights)
rsq_hridge_0001 <- hridge_model$rsq
print(rsq_hridge_0001)
prediction <- X_val %*% hridge_model$coef
mad <- mean(abs(as.vector(y_val) - prediction))
print(mad)
```

```{r}
model_init <- glmnet(X_train, y_train, alpha = 0, lambda = 0.001, standardize = FALSE)
betas_init <- as.vector(model_init$beta)
coef <- optim(betas_init, hridge_loss)$par
fitted <- X_train %*% coef
rsq <- cor(y_train, as.vector(fitted))^2
```

Cross Validation to get lambda
```{r}
result <- data.frame(lambda=numeric(), fold1=numeric(), fold2=numeric(), fold3=numeric(), fold4=numeric(), fold5=numeric(), fold6=numeric(), fold7=numeric(), fold8=numeric(), fold9=numeric(), fold10=numeric())
lambdas <- 10^seq(2, 8, length.out = 5)
nr_folds <- 10
folds <- createFolds(y_train, k = nr_folds)

for (lambda in lambdas){
  mad_fold <- c()
  for (i in 1:nr_folds){
    
    X_cv_train <- X_train[-folds[[i]],]
    y_cv_train <- y_train[-folds[[i]]]
    X_cv_test <- X_train[folds[[i]],]
    y_cv_test <- X_train[folds[[i]]]
    weights <- sapply(seq(ncol(X_cv_train)), function(predictor) {
      uni_model <- lm(y_cv_train ~ X_cv_train[, predictor])
      coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
      })
    hridge_loss <- function(betas) {
      sum((y_cv_train - X_cv_train %*% betas)^2) + lambda * sum(weights * betas^2)
      }
    model <- hridge(X_cv_train, y_cv_train, lambda = lambda, weights = weights)
    prediction <- X_cv_test %*% unname(model$coef)
    mad <- mean(abs(y_cv_test - prediction))
    mad_fold <- c(mad_fold, mad)
  }
  result <- rbind(result, c(lambda, mad_fold))
}
```

```{r}
base::rowMeans(result)
```

